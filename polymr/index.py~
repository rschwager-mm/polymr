import sys
import csv
import json
import logging
import multiprocessing
from zlib import compress as _compress
from base64 import b64encode
from heapq import merge as _merge
from collections import defaultdict

from toolz import partition_all

from . import storage
from . import record
from .util import ngrams


logger = logging.getLogger(__name__)


def features(rec):
    fs = set()
    for attr in rec:
        fs.update( ngrams(_compress(attr.encode())) )
    return fs


def _ef_worker(chunk):
    d = defaultdict(list)
    ksets = [ (i, features(row)) for i, row in chunk ]
    for i, kset in ksets:
        for kmer in kset:
            d[kmer].append(i)
    return { b64encode(kmer).decode() : rset
             for kmer, rset in d.items() }

        
def parse_input(f):
    do_close = False
    if type(f) is str:
        f = open(f)
        do_close = True
    try:
        for i, row in enumerate(csv.reader(f)):
            yield i, [field.strip().lower() for field in row[:-1]]
    finally:
        if do_close:
            f.close()


def extract_features(input_file, nproc, chunksize):
    pool = multiprocessing.Pool(nproc)
    chunks = partition_all(chunksize, parse_input(input_file))
    kmer_tables = pool.imap_unordered(_ef_worker, chunks, chunksize=1)
    for i, kmer_table in enumerate(kmer_tables):
        print(json.dumps(kmer_table))
        logging.info("processed %i records", i*chunksize)


def unpack(input_file, backend):
    alltoks = set()
    for i, tab in enumerate(map(json.loads, input_file)):
        alltoks.update(tab.keys())
        backend.save_tokprefix([ (tok, i, rng) for tok, rng in tab.items() ])
        logging.info("Processed %s tables", i)
    backend.save_toklist(list(alltoks))
    logging.info("Saved %s tokens", len(alltoks))


def _merge_to_range(ls):
    if len(ls) == 1:
        return ls[0]
    merged = _merge(*ls)
    ret = [next(merged)]
    prev = ret[0]
    compacted = False
    for x in merged:
        if x == prev + 1:
            if type(ret[-1]) is list:
                ret[-1][-1] = x
            else:
                ret[-1] = [prev, x]
                compacted = True
        else:
            ret.append(x)
        prev = x
    return ret, compacted
            

def organize(backend):
    alltoks = backend.get_toklist()
    tokfreqs = {}
    for tok in alltoks:
        keys, rngs = backend.get_tokprefix(tok)
        if not keys:
            continue
        logging.info("Merging tokens `%s' to `%s'", keys[0], keys[-1])
        rng, compacted = _merge_to_range(rngs)
        backend.save_token(tok, rng, compacted)
        tokfreqs[tok] = sum(x[1]-x[0]+1 if type(x) is list else 1 for x in rng)
        for k in keys:
            backend.delete(k)
        logging.info("Merged ranges for token `%s'", tok)
    backend.save_freqs(tokfreqs)


def records(input_file, backend):
    rs = record.from_csv(input_file)
    rowcount = backend.save_records(enumerate(rs))
    backend.save_rowcount(rowcount)



class CLI:

    name = "index"

    arguments = [
        (["-1", "--step1"], {
            "type": bool,
            "help": "Step 1: Generate feature sets from records"
        }),
        (["-2", "--step2"], {
            "type": bool,
            "help": "Step 2: Unpack feature sets into storage backend"
        }),
        (["-3", "--step3"], {
            "type": bool,
            "help": ("Step 3: Organize unpacked feature sets within storage"
                     " backend")
        }),
        storage.backend_arg,
        (["-r", "--records"], {
            "type": bool,
            "help": "Store primary keys and indexed attributes"
        }),
        (["-i", "--input"], {
            "help": "Defaults to stdin"
        }),
        (["-n", "--parallel"], {
            "type": int,
            "default": 1,
            "help": "Number of concurrent workers"
        }),
        (["--chunksize"], {
            "help": "Number of records for each worker to process in memory",
            "type": int,
            "default": 50000
        }),
    ]

    @staticmethod
    def hook(parser, args):
        if args.step1:
            inp = args.input or sys.stdin
            return extract_features(inp, args.parallel, args.chunksize)
        elif args.step2:
            pass
        elif args.step3:
            pass
        elif args.records:
            inp = args.input or sys.stdin
            backend = storage.parse_url(args.backend)
            return records(inp, backend)
        else:
            return parser.print_help()
        

